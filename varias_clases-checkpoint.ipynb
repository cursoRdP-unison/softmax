{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística para varias clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import Image  # Esto es para desplegar imágenes en la libreta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La base de datos a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión *softmax* (o tambien conocida como *regresión logística multinomial*) es el último de los algoritmos basado en modelos lineales generalizados que cubriremos en el curso de reconocimiento de patrones. Para ejemplificar su uso, vamos a utilizar una base de datos bastante comun, MNIST. \n",
    "\n",
    "MNIST es una base de datos de digitos escritos a mano, en formato de $20 \\times 20$ pixeles. La base completa puede obtenerse en la página de Yan LeCun (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Nosotros en realidad vamos a utilizar una base de datos reducida de la original y con imágenes de calidad más reducida ($16 \\times 16$ pixeles por imagen). Numpy provée un método para guardad objetos tipo numpy en un solo archivo, utilizando el método de compresión *gunzip*. Los datos ya se encuentran preprocesados y empaquetados en un archivo llamado `digitos.npz`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"digitos.npz\")\n",
    "\n",
    "print data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, `data` es un objeto contenedor de numpy cuyas llaves son `X_valida`, `X_entrena`, `T_valida`, `T_entrena`. Cada una de estas son a su vez objetos tipo ndarray de numpy, los cuales contienen valores de entrada y salida, tantopara entrenamiento como para validación. No se preocupen, esto de entrenamiento y validación lo vamos a ver más adelante en la clase.\n",
    "\n",
    "Cada renglon de x es una imagen *desenrrollada*, esto es los 256 datos de una imágen de $16 \\times 16$ pixeles. Por otra parte, cada renglon de y es un vector de 10 posiciones, donde todos los valores son ceros, salvo uno, que es el que define la clase de la imagen.\n",
    "\n",
    "Para darse una mejor idea, ejecuta el siguiente script varias veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = data['X_entrena']\n",
    "y = data['T_entrena']\n",
    "\n",
    "a = np.random.randint(0, y.shape[0])\n",
    "\n",
    "print \"-- x es de dimensiones \", x.shape\n",
    "print \"-- y es de dimensiones \", y.shape\n",
    "\n",
    "print \"\\ny si escogemos la imagen \", a, \"veremos\"\n",
    "\n",
    "plt.imshow(x[a,:].reshape(16,16), cmap=plt.gray())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print \"la cual es un\", list(y[a,:]).index(1)\n",
    "\n",
    "print\"\\n\\nY si miramos lo que contiene, veremos que\"\n",
    "print\"x[a,:] = \"\n",
    "print x[a,:]\n",
    "print \"y[a,:] = \"\n",
    "print y[a,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O bien, ejecuta este script para ver un grupo grande de imágenes (puedes hacer más grande la imagen para verla mejor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.arange(y.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "ind = indices[0:100].reshape(10,10)\n",
    "\n",
    "imagen = np.ones((10 * 16 + 4*11, 10 * 16 + 4*11))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        imagen[4 + i * 20: 20 + i * 20, 4 + j * 20: 20 + j * 20] = x[ind[i, j], :].reshape(16,16)\n",
    "        \n",
    "plt.imshow(imagen, cmap=plt.gray())\n",
    "plt.axis('off')\n",
    "plt.title(u\"Ejemplos aleatorios de imágenes a clasificar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, vamos a hacer una función que agregue la columna de unos para la x extendida, y simplificar más adelante su uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extendida(x):\n",
    "    \"\"\"\n",
    "    Agrega una columna de unos a x\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.c_[np.ones((x.shape[0], 1)), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regresión logística para varias clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la regresión logistica para varias clases, en lugar de estimar solamente un vector de parámetros, el valor de salida lo descomponemos en $K$ vectores de salida con valores de 1 (sipertenece a esa clase) o cero (si no pertenece). A esto se le conoce como *dummy variable*. En el ejemplo que tenemos, las clases ya se encuentran de esa forma (por eso la salida es un vector de dimensión 10 donde solo uno es 1 y todos los demás valores son 0).  Lo que tenemos que hacer es estimar una matriz de parámetros $\\omega$ tal que:\n",
    "\n",
    "$$\n",
    "\\omega = (\\omega^{(1)}, \\ldots, \\omega^{(K)}) \n",
    "$$\n",
    "\n",
    "donde $\\omega^{(k)} = (\\theta_0^{(k)}, \\ldots, \\omega_n^{(k)})^T$ es el vector columna que parametriza la clase $k$. De esta manera, $\\omega$ es ahora una matriz de dimensión $n+1 \\times K$. El aporte lineal a cada clase de un objeto $x^(i)$ está dado por\n",
    "\n",
    "$$\n",
    "z^{i} = \\omega^T x^{(i)},\n",
    "$$\n",
    "\n",
    "el cual es de dimensión $K \\times 1$ (un valor por cada clase). La probabilidad de pertenecer a la clase $k$ respecto al resto de las clases está dada por:\n",
    "\n",
    "$$\n",
    "\\hat{y}_k^{(i)} = logistica(z^{(i)}) = \\frac{1}{1 + \\exp(-z^{(i)})}.\n",
    "$$\n",
    "\n",
    "Como se puede ver, $\\hat{y}^{(i)}$ es independiente para cada posible valor de $k$, por lo que la suma no está condicionada a ser 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1. Con esta información, realiza la función *logistica*, de manera que si recibe un ndarray de dimensiones $ T \\times K$ con $T$ vectores, regrese la matriz de mismas dimensiones con el cálculo *logistica* para cada matriz (20 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Calculo de la regresión softmax\n",
    "    \n",
    "    @param z: ndarray de dimensión (T, K) donde z[i, :] es el vector de aportes lineales de el objeto i\n",
    "    \n",
    "    @return: un ndarray de dimensión (T, K) donde cada columna es el calculo softmax de su respectivo vector de entrada.\n",
    "    \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------------------------------------\n",
    "    result=1/(1+np.exp(-z))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora es necesario implementar la función de costo, la cual es la suma de los costos por cada salida de las k regresiones logísticas, la cual puede resumirse como:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{T}\\sum_{i=1}^T \\sum_{k=1}^K y_k^{(i)} \\log(\\hat{y}_k^{(i)}),\n",
    "$$\n",
    "\n",
    "donde $y_k^{(i)}$ es un valor de 0 o 1 dependiendo si el objeto $i$ pertenece a la clase $k$ o no, mientras que $\\hat{y}_k^{(i)}$ es la probabilidad que el objeto $i$ pertenezca a la clase $k$ conociendo $x^{(i)}$ y parametrizado por $\\omega$. Esto se vio en clase y no se realizará el repaso aqui en la libreta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2. Implementa la función de costo de manera relativamente eficiente, utilizando las facilidades que presenta numpy (20 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def costo(w, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el costo para la regresión softmax parametrizada por theta, \n",
    "    con el conjunto de datos dado por (x, y)\n",
    "    \n",
    "    @param w: ndarray de dimensión (n+1, K) con los parámetros\n",
    "    @param x: ndarray de dimensión (T, n+1) con los datos\n",
    "    @param y: ndarray de dimensión (T, K) con la clase por cada dato\n",
    "    \n",
    "    @return: Un valor flotante\n",
    "    \n",
    "    \"\"\"\n",
    "    T, K = y.shape\n",
    "    n = x.shape[1] - 1\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    # AGREGA AQUI TU CÓDIGO\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "def test_costo():\n",
    "    x = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "    y = np.eye(4)\n",
    "\n",
    "    w = np.array([[3, -4, -4],[-1, -1, 3], [.01, 3, -10], [-5, 5, 5]]).T\n",
    "    \n",
    "    assert costo(w, x, y) < 0.1\n",
    "    return \"Paso la prueba\"\n",
    "    \n",
    "print test_costo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3. Implementa la función para predecir el valor de y estimada sin tener que calcular la función softmax (20 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predice(w, x):\n",
    "    \"\"\"\n",
    "    Prediccion de y_hat a partir de la matriz theta para los valores de x\n",
    "    \n",
    "    @param w: ndarray de dimensión (n+1, K) con los parámetros\n",
    "    @param x: ndarray de dimensión (T, n+1) con los datos\n",
    "\n",
    "    @return: ndarray de dimensión (T, K) con la clase por cada dato (unos y ceros)\n",
    "    \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # AGREGA AQUI TU CÓDIGO\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "def prueba_prediccion():\n",
    "    x = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "    y = np.eye(4)\n",
    "\n",
    "    w = np.array([[3, -4, -4],[-1, -1, 3], [.01, 3, -10], [-5, 5, 5]]).T\n",
    "    \n",
    "    assert abs((y - predice(w, x)).sum()) < 1e-12 \n",
    "    print \"Paso la prueba\"\n",
    "    \n",
    "print prueba_prediccion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último tenemos que implementar el gradiente para poder utilizar los métodos de optimización (ya sea por descenso de gradiente o por algún método de optimización.\n",
    "\n",
    "El gradiente se obtiene a partir de las derivadas parciales:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\omega)}{\\partial \\omega_s^{(r)}} = - \\frac{1}{T} \\sum_{i = 1}^T \\left(y_k^{(i)} -  \\sum_{j=1}^K \\hat{y}_j^{i}\\right) x_s^{(i)},\n",
    "$$\n",
    "\n",
    "donde $k$ es la clase a la que pertenece el ejemplo $i$. Esto implica que cuando $k = r$, entonces solamente tenemos $1 - \\hat{y}_k^{(i)}$ para este dato. Si $k \\neq r$ entonces tendremos $-\\hat{y}_r^{(i)}$. \n",
    "\n",
    "Esto se puede resolver en forma matricial como\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = - \\frac{1}{T} X^T (Y - \\hat{Y})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4. Implementa el gradiente de la manera que menos se dificulte (15 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradiente(w, x, y):\n",
    "    \"\"\"\n",
    "    Calculo del gradiente para el problema de regresión softmax\n",
    "    \n",
    "    @param w: ndarray de dimensión (n+1, K) con los parámetros\n",
    "    @param x: ndarray de dimensión (T, n+1) con los datos\n",
    "    @param y: ndarray de dimensión (T, K) con la clase por cada dato\n",
    "    \n",
    "    @return: Un ndarray de mismas dimensiones que theta\n",
    "    \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # AGREGA AQUI TU CÓDIGO\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "def prueba_gradiente():\n",
    "    x = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "    y = np.eye(4)\n",
    "\n",
    "    w = np.array([[3, -4, -4],[-1, -1, 3], [.01, 3, -10], [-5, 5, 5]]).T\n",
    "    \n",
    "    print np.abs(g).max()\n",
    "    assert np.abs(g).max() < 0.05\n",
    "    \n",
    "    print \"Paso la prueba\"\n",
    "    \n",
    "print prueba_gradiente()    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, ya nos encontramos en capacidad para realizar el aprendizaje para la regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5. Implementa la regresión logística utilizando el método de descenso de gradiente (15 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dg_logistica_lotes(w, x, y, alpha=None, max_epoch=10000, epsilon=1e-3, errores=False):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente por lotes para la clasificación softmax\n",
    "    \n",
    "    \"\"\"\n",
    "    historial = np.zeros((max_epoch)) if errores else None\n",
    "        \n",
    "    for epoch in xrange(max_epoch):\n",
    "        #--------------------------------------------------------------------------------\n",
    "        # AGREGA AQUI TU CÓDIGO\n",
    "        #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #--------------------------------------------------------------------------------\n",
    "    return w, historial\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero para utilizar el descenso de gradiente hay que ajustar un valor de $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ajusta un valor de alpha razonable\n",
    "\n",
    "alfita = 1\n",
    "\n",
    "T, K = y.shape\n",
    "n = x.shape[1]\n",
    "\n",
    "w = 0.1 * (np.random.random((n + 1, K)) - 0.5)\n",
    "w, e_hist = dg_logistica_lotes(w, extendida(x), y, alpha=alfita, max_epoch=100, errores=True)\n",
    "plt.plot(e_hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y para probarlo vamos a aprender a clasificar a los digitos de nuestra base de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = 0.1 * (np.random.random((n + 1, K)) - 0.5)\n",
    "w, e_hist = dg_logistica_lotes(w, extendida(x), y, alpha=alfita, max_epoch=1000)\n",
    "\n",
    "print \"El costo de la solución final es de \", costo(w, extendida(x), y)\n",
    "\n",
    "y_estimada = predice(w, extendida(x))\n",
    "errores = np.where(y.argmax(axis=1) == y_estimada.argmax(axis=1), 0, 1)\n",
    "\n",
    "print \"\\nLos datos utilizados para el aprendizaje mal clasificados son el \", 100 * errores.mean(),\"%\"\n",
    "\n",
    "# Esto solo es para hacerla más emocionante\n",
    "x_test = data['X_valida']\n",
    "y_test = data['T_valida']\n",
    "y_estimada_T = predice(theta, extendida(x_test))\n",
    "errores = np.where(y_test.argmax(axis=1) == y_estimada_T.argmax(axis=1), 0, 1)\n",
    "\n",
    "print \"\\nY con los datos de pureba el error es del \", 100 * errores.mean(),\"%\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Será esta la mejor solución? ¿Será una buena solución? Por esto no hay que preocuparse mucho todavía, lo vamos a revisar más adelante en el curso. Se espera con la regresión en varias clases poder clasificar correctamente más del 95% de los datos de entrenamiento. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
